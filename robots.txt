# ===================================================================
# robots.txt for a Large-Scale Travel Booking Website
# ===================================================================

User-agent: *
# This section applies to all search engine crawlers.

# 1. Block private, user-specific, and action-oriented pages
Disallow: /admin/
Disallow: /login
Disallow: /register
Disallow: /my-account/
Disallow: /my-bookings/
Disallow: /checkout/
Disallow: /payment/
Disallow: /cart/
Disallow: /wishlist/

# 2. Block internal search results and API calls
Disallow: /search/internal/
Disallow: /api/

# 3. CRITICAL: Block faceted navigation and filtering parameters to save crawl budget
# This prevents Google from crawling millions of duplicate-content URLs.
# We allow the main destination search, but block URLs with extra filters.
Disallow: /*?*sort=
Disallow: /*?*filter=
Disallow: /*?*price=
Disallow: /*?*rating=
Disallow: /*?*amenities=
Disallow: /*?*property_type=
# Add any other filter parameters you use here. The asterisks (*) act as wildcards.

# 4. Allow crawling of important scripts and stylesheets if they are needed for rendering
# (This is often not necessary if they are on a CDN, but is good practice).
# Allow: /assets/css/
# Allow: /assets/js/

# ===================================================================

User-agent: Googlebot
# This section contains rules that apply ONLY to Google's main crawler.
# You can add specific rules here if needed, but for most cases, 
# the rules for '*' are sufficient.

# ===================================================================

User-agent: Googlebot-Image
# Allow crawling of all property images.
Allow: /images/properties/

# ===================================================================

# 5. Point all crawlers to the Sitemap Index file.
# This must be the full, absolute URL.
Sitemap: https://www.roompapa.com/sitemap_index.xml